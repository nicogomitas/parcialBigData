{"changed":true,"filter":false,"title":"app.py","tooltip":"/Parcial/app.py","value":"import requests\nimport boto3\nfrom datetime import datetime\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Crear un cliente de S3\ns3_client = boto3.client('s3')\n\n# Primera función Lambda: Descarga el contenido y lo sube a S3\ndef f(event, context):\n    bucket_name = \"parcial3-lambda1\"  # Bucket específico para esta función\n    base_path = \"headlines/raw\"\n\n    urls = {\n        \"el_tiempo\": \"https://www.eltiempo.com/\",\n        \"el_espectador\": \"https://www.elespectador.com/\",\n        \"publimetro\": \"https://www.publimetro.co/\"\n    }\n\n    today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n\n    for source, url in urls.items():\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            content = response.text\n\n            filename = f\"{base_path}/{source}-contenido-{today}.html\"\n\n            s3_client.put_object(\n                Bucket=bucket_name,\n                Key=filename,\n                Body=content,\n                ContentType=\"text/html\"\n            )\n            print(f\"Contenido de {source} subido correctamente a {filename}\")\n        except Exception as e:\n            print(f\"Error al procesar {source}: {e}\")\n\n\n# Segunda función Lambda: Procesa los datos de S3 y genera un CSV\n\ndef process_data(event, context):\n    bucket_name = \"parcial3-lambda1\"  # Bucket donde están los datos RAW\n    processed_bucket_name = \"parcial3-lambda2\"  # Bucket destino para el CSV\n    final_path = \"headlines/final\"\n\n    for record in event['Records']:\n        key = record['s3']['object']['key']\n        source = key.split('/')[-1].split('-')[0]  # Extraer el nombre del periódico\n\n        # Descargar el archivo desde S3\n        obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n        html_content = obj['Body'].read().decode('utf-8')\n\n        # Procesar el HTML con BeautifulSoup\n        soup = BeautifulSoup(html_content, 'html.parser')\n\n        headlines = []\n        for article in soup.find_all('article'):\n            title = article.find('h2') or article.find('h3')\n            link = article.find('a')\n            if title and link:\n                headlines.append({\n                    \"category\": source,\n                    \"headline\": title.get_text(strip=True),\n                    \"link\": link['href']\n                })\n\n        # Generar un archivo CSV\n        today = datetime.utcnow()\n        year, month, day = today.strftime(\"%Y\"), today.strftime(\"%m\"), today.strftime(\"%d\")\n        csv_filename = f\"{final_path}/periodico={source}/year={year}/month={month}/day={day}/headlines.csv\"\n\n        # Guardar los datos en el archivo CSV\n        csv_content = \"category,headline,link\\n\"\n        csv_content += \"\\n\".join([f\"{h['category']},{h['headline']},{h['link']}\" for h in headlines])\n\n        # Subir el archivo CSV a S3\n        s3_client.put_object(\n            Bucket=processed_bucket_name,\n            Key=csv_filename,\n            Body=csv_content,\n            ContentType=\"text/csv\"\n        )\n        print(f\"Archivo CSV subido correctamente a {csv_filename}\")\n        \n\n# Crear un cliente de Glue\nglue_client = boto3.client('glue')\n\ndef run_glue_crawler(event, context):\n    # Nombre del crawler en Glue\n    crawler_name = \"mi_crawler\"\n\n    try:\n        # Ejecutar el crawler\n        response = glue_client.start_crawler(Name=crawler_name)\n        print(f\"Crawler '{crawler_name}' iniciado correctamente: {response}\")\n    except glue_client.exceptions.CrawlerRunningException:\n        print(f\"El crawler '{crawler_name}' ya está en ejecución.\")\n    except Exception as e:\n        print(f\"Error al iniciar el crawler '{crawler_name}': {e}\")","undoManager":{"mark":4,"position":8,"stack":[[{"start":{"row":0,"column":0},"end":{"row":42,"column":0},"action":"insert","lines":["import requests","import boto3","from datetime import datetime","","# Crear un cliente de S3","s3_client = boto3.client('s3')","","def f(event, context):","    # Nombre del bucket de S3","    bucket_name = \"zappa-ktsbfja1q\"  # Cambia por el nombre de tu bucket","    base_path = \"headlines/raw\"","","    # URLs de las páginas a descargar","    urls = {","        \"el_tiempo\": \"https://www.eltiempo.com/\",","        \"el_espectador\": \"https://www.elespectador.com/\",","        \"publimetro\": \"https://www.publimetro.co/\"","    }","","    # Obtener la fecha actual en formato yyyy-mm-dd","    today = datetime.utcnow().strftime(\"%Y-%m-%d\")","","    for source, url in urls.items():","        try:","            # Descargar el contenido de la página","            response = requests.get(url)","            response.raise_for_status()  # Lanzar error si falla la solicitud","            content = response.text","","            # Crear el nombre del archivo en S3","            filename = f\"{base_path}/{source}-contenido-{today}.html\"","","            # Subir el contenido a S3","            s3_client.put_object(","                Bucket=bucket_name,","                Key=filename,","                Body=content,","                ContentType=\"text/html\"","            )","            print(f\"Contenido de {source} subido correctamente a {filename}\")","        except Exception as e:","            print(f\"Error al procesar {source}: {e}\")",""],"id":1}],[{"start":{"row":0,"column":0},"end":{"row":42,"column":0},"action":"remove","lines":["import requests","import boto3","from datetime import datetime","","# Crear un cliente de S3","s3_client = boto3.client('s3')","","def f(event, context):","    # Nombre del bucket de S3","    bucket_name = \"zappa-ktsbfja1q\"  # Cambia por el nombre de tu bucket","    base_path = \"headlines/raw\"","","    # URLs de las páginas a descargar","    urls = {","        \"el_tiempo\": \"https://www.eltiempo.com/\",","        \"el_espectador\": \"https://www.elespectador.com/\",","        \"publimetro\": \"https://www.publimetro.co/\"","    }","","    # Obtener la fecha actual en formato yyyy-mm-dd","    today = datetime.utcnow().strftime(\"%Y-%m-%d\")","","    for source, url in urls.items():","        try:","            # Descargar el contenido de la página","            response = requests.get(url)","            response.raise_for_status()  # Lanzar error si falla la solicitud","            content = response.text","","            # Crear el nombre del archivo en S3","            filename = f\"{base_path}/{source}-contenido-{today}.html\"","","            # Subir el contenido a S3","            s3_client.put_object(","                Bucket=bucket_name,","                Key=filename,","                Body=content,","                ContentType=\"text/html\"","            )","            print(f\"Contenido de {source} subido correctamente a {filename}\")","        except Exception as e:","            print(f\"Error al procesar {source}: {e}\")",""],"id":2},{"start":{"row":0,"column":0},"end":{"row":88,"column":0},"action":"insert","lines":["import requests","import boto3","from datetime import datetime","from bs4 import BeautifulSoup","import csv","import os","","# Crear un cliente de S3","s3_client = boto3.client('s3')","","# Primera función Lambda: Descarga el contenido y lo sube a S3","def f(event, context):","    bucket_name = \"zappa-ktsbfja1q\"  # Cambia por el nombre de tu bucket","    base_path = \"headlines/raw\"","","    urls = {","        \"el_tiempo\": \"https://www.eltiempo.com/\",","        \"el_espectador\": \"https://www.elespectador.com/\",","        \"publimetro\": \"https://www.publimetro.co/\"","    }","","    today = datetime.utcnow().strftime(\"%Y-%m-%d\")","","    for source, url in urls.items():","        try:","            response = requests.get(url)","            response.raise_for_status()","            content = response.text","","            filename = f\"{base_path}/{source}-contenido-{today}.html\"","","            s3_client.put_object(","                Bucket=bucket_name,","                Key=filename,","                Body=content,","                ContentType=\"text/html\"","            )","            print(f\"Contenido de {source} subido correctamente a {filename}\")","        except Exception as e:","            print(f\"Error al procesar {source}: {e}\")","","# Segunda función Lambda: Procesa los datos de S3 y genera un CSV","def process_data(event, context):","    bucket_name = \"zappa-ktsbfja1q\"  # Cambia por el nombre de tu bucket","    raw_path = \"headlines/raw\"","    final_path = \"headlines/final\"","","    # Obtener el archivo que activó el evento","    for record in event['Records']:","        key = record['s3']['object']['key']","        source = key.split('/')[-1].split('-')[0]  # Extraer el nombre del periódico","","        # Descargar el archivo desde S3","        obj = s3_client.get_object(Bucket=bucket_name, Key=key)","        html_content = obj['Body'].read().decode('utf-8')","","        # Procesar el HTML con BeautifulSoup","        soup = BeautifulSoup(html_content, 'html.parser')","","        # Extraer datos relevantes","        headlines = []","        for article in soup.find_all('article'):","            title = article.find('h2') or article.find('h3')","            link = article.find('a')","            if title and link:","                headlines.append({","                    \"category\": source,","                    \"headline\": title.get_text(strip=True),","                    \"link\": link['href']","                })","","        # Generar un archivo CSV","        today = datetime.utcnow()","        year, month, day = today.strftime(\"%Y\"), today.strftime(\"%m\"), today.strftime(\"%d\")","        csv_filename = f\"{final_path}/periodico={source}/year={year}/month={month}/day={day}/headlines.csv\"","","        # Guardar los datos en el archivo CSV","        csv_content = \"category,headline,link\\n\"","        csv_content += \"\\n\".join([f\"{h['category']},{h['headline']},{h['link']}\" for h in headlines])","","        # Subir el archivo CSV a S3","        s3_client.put_object(","            Bucket=bucket_name,","            Key=csv_filename,","            Body=csv_content,","            ContentType=\"text/csv\"","        )","        print(f\"Archivo CSV subido correctamente a {csv_filename}\")",""]}],[{"start":{"row":11,"column":0},"end":{"row":39,"column":53},"action":"remove","lines":["def f(event, context):","    bucket_name = \"zappa-ktsbfja1q\"  # Cambia por el nombre de tu bucket","    base_path = \"headlines/raw\"","","    urls = {","        \"el_tiempo\": \"https://www.eltiempo.com/\",","        \"el_espectador\": \"https://www.elespectador.com/\",","        \"publimetro\": \"https://www.publimetro.co/\"","    }","","    today = datetime.utcnow().strftime(\"%Y-%m-%d\")","","    for source, url in urls.items():","        try:","            response = requests.get(url)","            response.raise_for_status()","            content = response.text","","            filename = f\"{base_path}/{source}-contenido-{today}.html\"","","            s3_client.put_object(","                Bucket=bucket_name,","                Key=filename,","                Body=content,","                ContentType=\"text/html\"","            )","            print(f\"Contenido de {source} subido correctamente a {filename}\")","        except Exception as e:","            print(f\"Error al procesar {source}: {e}\")"],"id":3},{"start":{"row":11,"column":0},"end":{"row":40,"column":0},"action":"insert","lines":["def f(event, context):","    bucket_name = \"parcial3-lambda1\"  # Bucket específico para esta función","    base_path = \"headlines/raw\"","","    urls = {","        \"el_tiempo\": \"https://www.eltiempo.com/\",","        \"el_espectador\": \"https://www.elespectador.com/\",","        \"publimetro\": \"https://www.publimetro.co/\"","    }","","    today = datetime.utcnow().strftime(\"%Y-%m-%d\")","","    for source, url in urls.items():","        try:","            response = requests.get(url)","            response.raise_for_status()","            content = response.text","","            filename = f\"{base_path}/{source}-contenido-{today}.html\"","","            s3_client.put_object(","                Bucket=bucket_name,","                Key=filename,","                Body=content,","                ContentType=\"text/html\"","            )","            print(f\"Contenido de {source} subido correctamente a {filename}\")","        except Exception as e:","            print(f\"Error al procesar {source}: {e}\")",""]}],[{"start":{"row":43,"column":0},"end":{"row":89,"column":0},"action":"remove","lines":["def process_data(event, context):","    bucket_name = \"zappa-ktsbfja1q\"  # Cambia por el nombre de tu bucket","    raw_path = \"headlines/raw\"","    final_path = \"headlines/final\"","","    # Obtener el archivo que activó el evento","    for record in event['Records']:","        key = record['s3']['object']['key']","        source = key.split('/')[-1].split('-')[0]  # Extraer el nombre del periódico","","        # Descargar el archivo desde S3","        obj = s3_client.get_object(Bucket=bucket_name, Key=key)","        html_content = obj['Body'].read().decode('utf-8')","","        # Procesar el HTML con BeautifulSoup","        soup = BeautifulSoup(html_content, 'html.parser')","","        # Extraer datos relevantes","        headlines = []","        for article in soup.find_all('article'):","            title = article.find('h2') or article.find('h3')","            link = article.find('a')","            if title and link:","                headlines.append({","                    \"category\": source,","                    \"headline\": title.get_text(strip=True),","                    \"link\": link['href']","                })","","        # Generar un archivo CSV","        today = datetime.utcnow()","        year, month, day = today.strftime(\"%Y\"), today.strftime(\"%m\"), today.strftime(\"%d\")","        csv_filename = f\"{final_path}/periodico={source}/year={year}/month={month}/day={day}/headlines.csv\"","","        # Guardar los datos en el archivo CSV","        csv_content = \"category,headline,link\\n\"","        csv_content += \"\\n\".join([f\"{h['category']},{h['headline']},{h['link']}\" for h in headlines])","","        # Subir el archivo CSV a S3","        s3_client.put_object(","            Bucket=bucket_name,","            Key=csv_filename,","            Body=csv_content,","            ContentType=\"text/csv\"","        )","        print(f\"Archivo CSV subido correctamente a {csv_filename}\")",""],"id":5},{"start":{"row":43,"column":0},"end":{"row":82,"column":0},"action":"insert","lines":["def process_data(event, context):","    bucket_name = \"parcial3-lambda2\"  # Bucket específico para esta función","    raw_path = \"headlines/raw\"","    final_path = \"headlines/final\"","","    for record in event['Records']:","        key = record['s3']['object']['key']","        source = key.split('/')[-1].split('-')[0]  # Extraer el nombre del periódico","","        obj = s3_client.get_object(Bucket=\"parcial3-lambda1\", Key=key)  # Cambia a `parcial3-lambda1`","        html_content = obj['Body'].read().decode('utf-8')","","        soup = BeautifulSoup(html_content, 'html.parser')","","        headlines = []","        for article in soup.find_all('article'):","            title = article.find('h2') or article.find('h3')","            link = article.find('a')","            if title and link:","                headlines.append({","                    \"category\": source,","                    \"headline\": title.get_text(strip=True),","                    \"link\": link['href']","                })","","        today = datetime.utcnow()","        year, month, day = today.strftime(\"%Y\"), today.strftime(\"%m\"), today.strftime(\"%d\")","        csv_filename = f\"{final_path}/periodico={source}/year={year}/month={month}/day={day}/headlines.csv\"","","        csv_content = \"category,headline,link\\n\"","        csv_content += \"\\n\".join([f\"{h['category']},{h['headline']},{h['link']}\" for h in headlines])","","        s3_client.put_object(","            Bucket=bucket_name,","            Key=csv_filename,","            Body=csv_content,","            ContentType=\"text/csv\"","        )","        print(f\"Archivo CSV subido correctamente a {csv_filename}\")",""]}],[{"start":{"row":43,"column":0},"end":{"row":82,"column":0},"action":"remove","lines":["def process_data(event, context):","    bucket_name = \"parcial3-lambda2\"  # Bucket específico para esta función","    raw_path = \"headlines/raw\"","    final_path = \"headlines/final\"","","    for record in event['Records']:","        key = record['s3']['object']['key']","        source = key.split('/')[-1].split('-')[0]  # Extraer el nombre del periódico","","        obj = s3_client.get_object(Bucket=\"parcial3-lambda1\", Key=key)  # Cambia a `parcial3-lambda1`","        html_content = obj['Body'].read().decode('utf-8')","","        soup = BeautifulSoup(html_content, 'html.parser')","","        headlines = []","        for article in soup.find_all('article'):","            title = article.find('h2') or article.find('h3')","            link = article.find('a')","            if title and link:","                headlines.append({","                    \"category\": source,","                    \"headline\": title.get_text(strip=True),","                    \"link\": link['href']","                })","","        today = datetime.utcnow()","        year, month, day = today.strftime(\"%Y\"), today.strftime(\"%m\"), today.strftime(\"%d\")","        csv_filename = f\"{final_path}/periodico={source}/year={year}/month={month}/day={day}/headlines.csv\"","","        csv_content = \"category,headline,link\\n\"","        csv_content += \"\\n\".join([f\"{h['category']},{h['headline']},{h['link']}\" for h in headlines])","","        s3_client.put_object(","            Bucket=bucket_name,","            Key=csv_filename,","            Body=csv_content,","            ContentType=\"text/csv\"","        )","        print(f\"Archivo CSV subido correctamente a {csv_filename}\")",""],"id":6},{"start":{"row":43,"column":0},"end":{"row":87,"column":67},"action":"insert","lines":["","def process_data(event, context):","    bucket_name = \"parcial3-lambda1\"  # Bucket donde están los datos RAW","    processed_bucket_name = \"parcial3-lambda2\"  # Bucket destino para el CSV","    final_path = \"headlines/final\"","","    for record in event['Records']:","        key = record['s3']['object']['key']","        source = key.split('/')[-1].split('-')[0]  # Extraer el nombre del periódico","","        # Descargar el archivo desde S3","        obj = s3_client.get_object(Bucket=bucket_name, Key=key)","        html_content = obj['Body'].read().decode('utf-8')","","        # Procesar el HTML con BeautifulSoup","        soup = BeautifulSoup(html_content, 'html.parser')","","        headlines = []","        for article in soup.find_all('article'):","            title = article.find('h2') or article.find('h3')","            link = article.find('a')","            if title and link:","                headlines.append({","                    \"category\": source,","                    \"headline\": title.get_text(strip=True),","                    \"link\": link['href']","                })","","        # Generar un archivo CSV","        today = datetime.utcnow()","        year, month, day = today.strftime(\"%Y\"), today.strftime(\"%m\"), today.strftime(\"%d\")","        csv_filename = f\"{final_path}/periodico={source}/year={year}/month={month}/day={day}/headlines.csv\"","","        # Guardar los datos en el archivo CSV","        csv_content = \"category,headline,link\\n\"","        csv_content += \"\\n\".join([f\"{h['category']},{h['headline']},{h['link']}\" for h in headlines])","","        # Subir el archivo CSV a S3","        s3_client.put_object(","            Bucket=processed_bucket_name,","            Key=csv_filename,","            Body=csv_content,","            ContentType=\"text/csv\"","        )","        print(f\"Archivo CSV subido correctamente a {csv_filename}\")"]}],[{"start":{"row":87,"column":67},"end":{"row":88,"column":0},"action":"insert","lines":["",""],"id":7},{"start":{"row":88,"column":0},"end":{"row":88,"column":8},"action":"insert","lines":["        "]},{"start":{"row":88,"column":8},"end":{"row":89,"column":0},"action":"insert","lines":["",""]},{"start":{"row":89,"column":0},"end":{"row":89,"column":8},"action":"insert","lines":["        "]}],[{"start":{"row":89,"column":4},"end":{"row":89,"column":8},"action":"remove","lines":["    "],"id":8},{"start":{"row":89,"column":0},"end":{"row":89,"column":4},"action":"remove","lines":["    "]},{"start":{"row":88,"column":8},"end":{"row":89,"column":0},"action":"remove","lines":["",""]},{"start":{"row":88,"column":4},"end":{"row":88,"column":8},"action":"remove","lines":["    "]},{"start":{"row":88,"column":0},"end":{"row":88,"column":4},"action":"remove","lines":["    "]}],[{"start":{"row":88,"column":0},"end":{"row":103,"column":67},"action":"insert","lines":["","# Crear un cliente de Glue","glue_client = boto3.client('glue')","","def run_glue_crawler(event, context):","    # Nombre del crawler en Glue","    crawler_name = \"mi_crawler\"","","    try:","        # Ejecutar el crawler","        response = glue_client.start_crawler(Name=crawler_name)","        print(f\"Crawler '{crawler_name}' iniciado correctamente: {response}\")","    except glue_client.exceptions.CrawlerRunningException:","        print(f\"El crawler '{crawler_name}' ya está en ejecución.\")","    except Exception as e:","        print(f\"Error al iniciar el crawler '{crawler_name}': {e}\")"],"id":9}],[{"start":{"row":87,"column":67},"end":{"row":88,"column":0},"action":"insert","lines":["",""],"id":10},{"start":{"row":88,"column":0},"end":{"row":88,"column":8},"action":"insert","lines":["        "]}]]},"ace":{"folds":[],"scrolltop":720,"scrollleft":0,"selection":{"start":{"row":0,"column":0},"end":{"row":104,"column":67},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1732202629483}